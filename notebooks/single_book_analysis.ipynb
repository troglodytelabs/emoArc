{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Book Emotion Analysis Test\n",
    "\n",
    "Test the complete pipeline on a single book:\n",
    "1. Download from Project Gutenberg\n",
    "2. Detect chapters with improved patterns\n",
    "3. Run RoBERTa emotion predictions per chapter\n",
    "4. Visualize emotional trajectory\n",
    "5. Generate narrative analysis"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Check if dependencies are installed, if not install them\nimport sys\nimport subprocess\n\ndef check_and_install_packages():\n    \"\"\"Check for required packages and install if missing.\"\"\"\n    required = {\n        'torch': 'torch',\n        'transformers': 'transformers',\n        'numpy': 'numpy',\n        'pandas': 'pandas',\n        'matplotlib': 'matplotlib',\n        'seaborn': 'seaborn',\n        'requests': 'requests'\n    }\n    \n    missing = []\n    for module, package in required.items():\n        try:\n            __import__(module)\n        except ImportError:\n            missing.append(package)\n    \n    if missing:\n        print(f\"Installing missing packages: {', '.join(missing)}\")\n        print(\"This may take a few minutes...\")\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + missing)\n        print(\"âœ“ Installation complete!\")\n    else:\n        print(\"âœ“ All required packages are already installed!\")\n\ncheck_and_install_packages()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup - Install Dependencies\n\nRun this cell first to ensure all required packages are installed.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src' / 'analysis'))\n",
    "from emoPredict_roberta import PlutchikEmotionClassifier, load_model, predict_emotions\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book configuration\n",
    "BOOK_ID = 2701  # Moby Dick for testing improved chapter detection\n",
    "BOOK_TITLE = \"Moby Dick\"\n",
    "BOOK_AUTHOR = \"Herman Melville\"\n",
    "\n",
    "# Model path (adjust for your local setup)\n",
    "MODEL_PATH = \"/Users/devindyson/Desktop/troglodytelabs/emoWork/emoBERT/models/best_model.pt\"\n",
    "\n",
    "# Processing options\n",
    "MAX_CHAPTERS = 10  # Limit for testing (set to None for full book)\n",
    "SEGMENT_METHOD = \"chapter\"  # \"chapter\" or \"fixed_chunk\"\n",
    "CHUNK_SIZE = 500  # Words per chunk if using fixed_chunk\n",
    "\n",
    "print(f\"Testing: {BOOK_TITLE} (ID: {BOOK_ID})\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Max chapters: {MAX_CHAPTERS if MAX_CHAPTERS else 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gutenberg_book(book_id, max_retries=3):\n",
    "    \"\"\"Download a book from Project Gutenberg with fallback URLs.\"\"\"\n",
    "    urls = [\n",
    "        f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt',\n",
    "        f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt',\n",
    "        f'https://www.gutenberg.org/ebooks/{book_id}.txt.utf-8'\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"  Trying: {url} (attempt {attempt + 1}/{max_retries})\")\n",
    "                response = requests.get(url, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    try:\n",
    "                        text = response.content.decode('utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        text = response.content.decode('latin-1')\n",
    "                    \n",
    "                    print(f\"  âœ“ Downloaded {len(text):,} characters\")\n",
    "                    return text\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Downloading book...\")\n",
    "raw_text = download_gutenberg_book(BOOK_ID)\n",
    "\n",
    "if raw_text:\n",
    "    print(f\"\\nâœ“ Download successful!\")\n",
    "    print(f\"Total characters: {len(raw_text):,}\")\n",
    "else:\n",
    "    print(\"\\nâœ— Download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview raw text\n",
    "print(\"First 2000 characters:\")\n",
    "print(\"=\"*80)\n",
    "print(raw_text[:2000])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Strip Gutenberg Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_gutenberg_boilerplate(text):\n",
    "    \"\"\"Remove Project Gutenberg header and footer boilerplate.\"\"\"\n",
    "    start_patterns = [\n",
    "        r'\\*\\*\\* START OF TH(IS|E) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "        r'\\*\\*\\*START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "    ]\n",
    "    \n",
    "    start_pos = 0\n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start_pos = match.end()\n",
    "            break\n",
    "    \n",
    "    end_patterns = [\n",
    "        r'\\*\\*\\* END OF TH(IS|E) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "        r'\\*\\*\\*END OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "    ]\n",
    "    \n",
    "    end_pos = len(text)\n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            end_pos = match.start()\n",
    "            break\n",
    "    \n",
    "    return text[start_pos:end_pos].strip()\n",
    "\n",
    "clean_text = strip_gutenberg_boilerplate(raw_text)\n",
    "removed_pct = (len(raw_text) - len(clean_text)) / len(raw_text) * 100\n",
    "\n",
    "print(f\"Removed {removed_pct:.1f}% boilerplate\")\n",
    "print(f\"Clean text: {len(clean_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview clean text\n",
    "print(\"First 2000 characters of clean text:\")\n",
    "print(\"=\"*80)\n",
    "print(clean_text[:2000])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improved Chapter Detection\n",
    "\n",
    "Enhanced patterns to handle various chapter formats including titles (e.g., \"CHAPTER 1. Loomings.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_chapters_improved(text, verbose=True):\n",
    "    \"\"\"\n",
    "    Improved chapter detection with support for chapter titles.\n",
    "    \n",
    "    Handles formats like:\n",
    "    - \"CHAPTER 1. Loomings.\"\n",
    "    - \"CHAPTER I\"\n",
    "    - \"Chapter 1: The Beginning\"\n",
    "    \"\"\"\n",
    "    # Enhanced patterns (order matters - more specific first)\n",
    "    patterns = [\n",
    "        # With titles after number/numeral\n",
    "        (r'^CHAPTER\\s+[IVXLCDM]+[\\.:)]?\\s+.+$', 'CHAPTER + Roman + Title'),\n",
    "        (r'^CHAPTER\\s+\\d+[\\.:)]?\\s+.+$', 'CHAPTER + Arabic + Title'),\n",
    "        (r'^Chapter\\s+[IVXLCDM]+[\\.:)]?\\s+.+$', 'Chapter + Roman + Title'),\n",
    "        (r'^Chapter\\s+\\d+[\\.:)]?\\s+.+$', 'Chapter + Arabic + Title'),\n",
    "        \n",
    "        # Without titles\n",
    "        (r'^CHAPTER\\s+[IVXLCDM]+\\.?\\s*$', 'CHAPTER + Roman'),\n",
    "        (r'^CHAPTER\\s+\\d+\\.?\\s*$', 'CHAPTER + Arabic'),\n",
    "        (r'^Chapter\\s+[IVXLCDM]+\\.?\\s*$', 'Chapter + Roman'),\n",
    "        (r'^Chapter\\s+\\d+\\.?\\s*$', 'Chapter + Arabic'),\n",
    "        \n",
    "        # Just numerals/numbers (risky, only if nothing else works)\n",
    "        (r'^[IVXLCDM]+\\.?\\s*$', 'Roman only'),\n",
    "        (r'^\\d+\\.?\\s*$', 'Arabic only'),\n",
    "    ]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Try each pattern\n",
    "    for pattern, pattern_name in patterns:\n",
    "        matches = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if re.match(pattern, line, re.MULTILINE):\n",
    "                # Calculate character position\n",
    "                char_pos = sum(len(l) + 1 for l in lines[:i])\n",
    "                matches.append((line, char_pos, i))\n",
    "        \n",
    "        # Use this pattern if we found a reasonable number of chapters\n",
    "        if 3 <= len(matches) <= 300:  # Expanded upper limit for long books\n",
    "            if verbose:\n",
    "                print(f\"âœ“ Pattern matched: {pattern_name}\")\n",
    "                print(f\"  Pattern: {pattern}\")\n",
    "                print(f\"  Found: {len(matches)} chapters\")\n",
    "                print(f\"  Sample chapters:\")\n",
    "                for i, (title, _, _) in enumerate(matches[:5]):\n",
    "                    print(f\"    {i+1}. {title}\")\n",
    "                if len(matches) > 5:\n",
    "                    print(f\"    ... and {len(matches)-5} more\")\n",
    "            \n",
    "            # Extract chapter texts\n",
    "            chapters = []\n",
    "            for i, (title, pos, line_num) in enumerate(matches):\n",
    "                # Get text until next chapter (or end)\n",
    "                if i < len(matches) - 1:\n",
    "                    next_pos = matches[i + 1][1]\n",
    "                    chapter_text = text[pos:next_pos]\n",
    "                else:\n",
    "                    chapter_text = text[pos:]\n",
    "                \n",
    "                chapters.append((title, chapter_text, pos))\n",
    "            \n",
    "            return chapters, pattern_name\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"âœ— No reliable chapter pattern found\")\n",
    "        print(\"  Trying to show potential chapter lines...\")\n",
    "        chapter_like = []\n",
    "        for i, line in enumerate(lines[:1000]):  # Check first 1000 lines\n",
    "            line = line.strip()\n",
    "            if 'chapter' in line.lower() or 'CHAPTER' in line:\n",
    "                chapter_like.append(f\"  Line {i}: {line[:80]}\")\n",
    "        if chapter_like:\n",
    "            print(\"  Lines containing 'chapter':\")\n",
    "            for cl in chapter_like[:10]:\n",
    "                print(cl)\n",
    "    \n",
    "    return [], None\n",
    "\n",
    "# Test chapter detection\n",
    "print(\"Testing improved chapter detection...\\n\")\n",
    "chapters, pattern_used = detect_chapters_improved(clean_text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chapter statistics\n",
    "if chapters:\n",
    "    chapter_stats = []\n",
    "    for i, (title, text, pos) in enumerate(chapters, 1):\n",
    "        word_count = len(text.split())\n",
    "        char_count = len(text)\n",
    "        chapter_stats.append({\n",
    "            'chapter': i,\n",
    "            'title': title,\n",
    "            'words': word_count,\n",
    "            'characters': char_count\n",
    "        })\n",
    "    \n",
    "    df_chapters = pd.DataFrame(chapter_stats)\n",
    "    \n",
    "    print(\"\\nChapter Statistics:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total chapters: {len(chapters)}\")\n",
    "    print(f\"Average words per chapter: {df_chapters['words'].mean():.0f} (Â±{df_chapters['words'].std():.0f})\")\n",
    "    print(f\"Min words: {df_chapters['words'].min()}\")\n",
    "    print(f\"Max words: {df_chapters['words'].max()}\")\n",
    "    print(f\"Median words: {df_chapters['words'].median():.0f}\")\n",
    "    \n",
    "    # Show first 10 chapters\n",
    "    print(\"\\nFirst 10 chapters:\")\n",
    "    print(df_chapters.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualize distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(df_chapters['words'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(df_chapters['words'].mean(), color='r', linestyle='--', label='Mean')\n",
    "    plt.axvline(df_chapters['words'].median(), color='g', linestyle='--', label='Median')\n",
    "    plt.xlabel('Words per Chapter')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Chapter Length Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df_chapters['chapter'], df_chapters['words'], 'o-', alpha=0.7)\n",
    "    plt.xlabel('Chapter Number')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title('Chapter Length Progression')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nâœ— No chapters detected - will need to use fixed-chunk segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load RoBERTa Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model from {MODEL_PATH}...\")\n",
    "if Path(MODEL_PATH).exists():\n",
    "    model, thresholds = load_model(MODEL_PATH, device)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    print(\"\\nâœ“ Model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"\\nâœ— Model not found at {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH in the configuration cell\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Emotion Predictions Per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model and chapters:\n",
    "    # Limit chapters for testing\n",
    "    chapters_to_process = chapters[:MAX_CHAPTERS] if MAX_CHAPTERS else chapters\n",
    "    \n",
    "    print(f\"Processing {len(chapters_to_process)} chapters...\\n\")\n",
    "    \n",
    "    emotional_trajectory = []\n",
    "    \n",
    "    for i, (title, text, pos) in enumerate(chapters_to_process, 1):\n",
    "        # Get word count\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        # Predict emotions\n",
    "        results = predict_emotions(text, model, tokenizer, thresholds, device)\n",
    "        \n",
    "        # Format emotion scores\n",
    "        emotions = {r[\"emotion\"]: r[\"probability\"] for r in results}\n",
    "        \n",
    "        # Find dominant emotion\n",
    "        dominant = max(results, key=lambda x: x[\"probability\"])\n",
    "        \n",
    "        # Add to trajectory\n",
    "        emotional_trajectory.append({\n",
    "            \"chapter\": i,\n",
    "            \"title\": title,\n",
    "            \"word_count\": word_count,\n",
    "            \"emotions\": emotions,\n",
    "            \"dominant_emotion\": dominant[\"emotion\"],\n",
    "            \"intensity\": dominant[\"probability\"]\n",
    "        })\n",
    "        \n",
    "        # Progress\n",
    "        if i % 5 == 0 or i == len(chapters_to_process):\n",
    "            print(f\"  Processed {i}/{len(chapters_to_process)} chapters...\")\n",
    "    \n",
    "    print(\"\\nâœ“ Emotion predictions complete!\")\n",
    "else:\n",
    "    print(\"Cannot run predictions - model not loaded or no chapters detected\")\n",
    "    emotional_trajectory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Chapter Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if emotional_trajectory:\n",
    "    # Create detailed breakdown\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"PER-CHAPTER EMOTIONAL BREAKDOWN\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for seg in emotional_trajectory:\n",
    "        print(f\"\\nChapter {seg['chapter']}: {seg['title']}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"  Words: {seg['word_count']:,}\")\n",
    "        print(f\"  Dominant Emotion: {seg['dominant_emotion']} (intensity: {seg['intensity']:.2%})\")\n",
    "        print(f\"  All Emotions:\")\n",
    "        \n",
    "        # Sort emotions by score\n",
    "        sorted_emotions = sorted(seg['emotions'].items(), key=lambda x: -x[1])\n",
    "        for emotion, score in sorted_emotions:\n",
    "            bar = 'â–ˆ' * int(score * 50)  # Visual bar\n",
    "            marker = 'â˜…' if emotion == seg['dominant_emotion'] else ' '\n",
    "            print(f\"    {marker} {emotion:15s}: {score:5.1%} {bar}\")\n",
    "else:\n",
    "    print(\"No emotional trajectory data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Emotional Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if emotional_trajectory:\n",
    "    # Prepare data for visualization\n",
    "    chapters_num = [seg['chapter'] for seg in emotional_trajectory]\n",
    "    \n",
    "    # Extract emotion scores\n",
    "    emotion_names = list(emotional_trajectory[0]['emotions'].keys())\n",
    "    emotion_data = {emotion: [] for emotion in emotion_names}\n",
    "    \n",
    "    for seg in emotional_trajectory:\n",
    "        for emotion in emotion_names:\n",
    "            emotion_data[emotion].append(seg['emotions'][emotion])\n",
    "    \n",
    "    # Plot 1: All emotions over chapters\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # 1. All emotions line plot\n",
    "    ax = axes[0, 0]\n",
    "    for emotion in emotion_names:\n",
    "        ax.plot(chapters_num, emotion_data[emotion], marker='o', label=emotion, alpha=0.7)\n",
    "    ax.set_xlabel('Chapter')\n",
    "    ax.set_ylabel('Emotion Intensity')\n",
    "    ax.set_title('Emotional Trajectory - All Emotions')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Dominant emotion intensity\n",
    "    ax = axes[0, 1]\n",
    "    intensities = [seg['intensity'] for seg in emotional_trajectory]\n",
    "    dominant_emotions = [seg['dominant_emotion'] for seg in emotional_trajectory]\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(set(dominant_emotions))))\n",
    "    emotion_colors = dict(zip(set(dominant_emotions), colors))\n",
    "    \n",
    "    for i, (chapter, intensity, emotion) in enumerate(zip(chapters_num, intensities, dominant_emotions)):\n",
    "        ax.bar(chapter, intensity, color=emotion_colors[emotion], alpha=0.7)\n",
    "    \n",
    "    # Create legend\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=emotion_colors[e]) for e in sorted(set(dominant_emotions))]\n",
    "    ax.legend(handles, sorted(set(dominant_emotions)), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.set_xlabel('Chapter')\n",
    "    ax.set_ylabel('Intensity')\n",
    "    ax.set_title('Dominant Emotion Intensity per Chapter')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Emotion distribution\n",
    "    ax = axes[1, 0]\n",
    "    emotion_counts = {}\n",
    "    for seg in emotional_trajectory:\n",
    "        emotion = seg['dominant_emotion']\n",
    "        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "    \n",
    "    emotions_sorted = sorted(emotion_counts.items(), key=lambda x: -x[1])\n",
    "    ax.bar([e[0] for e in emotions_sorted], [e[1] for e in emotions_sorted], alpha=0.7)\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Number of Chapters')\n",
    "    ax.set_title('Dominant Emotion Distribution')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Heatmap of all emotions\n",
    "    ax = axes[1, 1]\n",
    "    emotion_matrix = np.array([emotion_data[e] for e in emotion_names]).T\n",
    "    im = ax.imshow(emotion_matrix, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    ax.set_ylabel('Chapter')\n",
    "    ax.set_title('Emotion Heatmap')\n",
    "    ax.set_xticks(range(len(emotion_names)))\n",
    "    ax.set_xticklabels(emotion_names, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(chapters_num)))\n",
    "    ax.set_yticklabels(chapters_num)\n",
    "    plt.colorbar(im, ax=ax, label='Intensity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Narrative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if emotional_trajectory:\n",
    "    # Calculate narrative features\n",
    "    intensities = [seg['intensity'] for seg in emotional_trajectory]\n",
    "    climax_idx = np.argmax(intensities)\n",
    "    climax_chapter = emotional_trajectory[climax_idx]\n",
    "    \n",
    "    # Emotional range (variance across emotions)\n",
    "    all_emotion_scores = {}\n",
    "    for seg in emotional_trajectory:\n",
    "        for emotion, score in seg['emotions'].items():\n",
    "            if emotion not in all_emotion_scores:\n",
    "                all_emotion_scores[emotion] = []\n",
    "            all_emotion_scores[emotion].append(score)\n",
    "    \n",
    "    emotional_range = np.mean([np.var(scores) for scores in all_emotion_scores.values()])\n",
    "    \n",
    "    # Pacing (rate of dominant emotion change)\n",
    "    emotion_changes = sum(\n",
    "        1 for i in range(1, len(emotional_trajectory))\n",
    "        if emotional_trajectory[i]['dominant_emotion'] != emotional_trajectory[i-1]['dominant_emotion']\n",
    "    )\n",
    "    pacing_score = emotion_changes / len(emotional_trajectory)\n",
    "    \n",
    "    # Average intensity\n",
    "    avg_intensity = np.mean(intensities)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NARRATIVE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nBook: {BOOK_TITLE} by {BOOK_AUTHOR}\")\n",
    "    print(f\"Chapters analyzed: {len(emotional_trajectory)}\")\n",
    "    print(f\"Pattern used: {pattern_used}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ CLIMAX:\")\n",
    "    print(f\"  Chapter {climax_chapter['chapter']}: {climax_chapter['title']}\")\n",
    "    print(f\"  Emotion: {climax_chapter['dominant_emotion']}\")\n",
    "    print(f\"  Intensity: {climax_chapter['intensity']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š EMOTIONAL CHARACTERISTICS:\")\n",
    "    print(f\"  Average intensity: {avg_intensity:.2%}\")\n",
    "    print(f\"  Emotional range (variance): {emotional_range:.4f}\")\n",
    "    print(f\"  Pacing score (emotion changes): {pacing_score:.2%}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ­ DOMINANT EMOTION BREAKDOWN:\")\n",
    "    for emotion, count in sorted(emotion_counts.items(), key=lambda x: -x[1]):\n",
    "        pct = count / len(emotional_trajectory) * 100\n",
    "        print(f\"  {emotion:15s}: {count:3d} chapters ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"No narrative analysis available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if emotional_trajectory:\n",
    "    # Prepare output\n",
    "    output = {\n",
    "        \"book_metadata\": {\n",
    "            \"book_id\": f\"gutenberg:{BOOK_ID}\",\n",
    "            \"title\": BOOK_TITLE,\n",
    "            \"author\": BOOK_AUTHOR,\n",
    "            \"total_chapters\": len(chapters),\n",
    "            \"processed_chapters\": len(emotional_trajectory),\n",
    "            \"pattern_used\": pattern_used\n",
    "        },\n",
    "        \"emotional_trajectory\": emotional_trajectory,\n",
    "        \"narrative_features\": {\n",
    "            \"climax_chapter\": climax_chapter['chapter'],\n",
    "            \"climax_title\": climax_chapter['title'],\n",
    "            \"climax_emotion\": climax_chapter['dominant_emotion'],\n",
    "            \"climax_intensity\": climax_chapter['intensity'],\n",
    "            \"average_intensity\": float(avg_intensity),\n",
    "            \"emotional_range\": float(emotional_range),\n",
    "            \"pacing_score\": float(pacing_score)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    output_dir = Path('../output')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    filename = f\"{BOOK_TITLE.lower().replace(' ', '_')}_analysis.json\"\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Results saved to: {output_path}\")\n",
    "    print(f\"\\nFile size: {output_path.stat().st_size:,} bytes\")\n",
    "else:\n",
    "    print(\"No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ“ Book downloaded: {BOOK_TITLE}\")\n",
    "print(f\"âœ“ Chapter detection: {len(chapters) if chapters else 0} chapters found\")\n",
    "if pattern_used:\n",
    "    print(f\"  Pattern: {pattern_used}\")\n",
    "print(f\"âœ“ Emotion predictions: {len(emotional_trajectory)} chapters processed\")\n",
    "print(f\"âœ“ Visualizations: Generated\")\n",
    "if emotional_trajectory:\n",
    "    print(f\"âœ“ Results saved: output/{filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Test complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}