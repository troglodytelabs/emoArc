{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg Data Validation for Emotional Story Arcs\n",
    "\n",
    "This notebook validates whether Project Gutenberg books are suitable for our emotional story arc analysis pipeline.\n",
    "\n",
    "## Goals\n",
    "1. Download 5 diverse books from Project Gutenberg\n",
    "2. Test chapter detection patterns\n",
    "3. Analyze text quality and structure\n",
    "4. Validate segment viability for RoBERTa emotion model (128 tokens)\n",
    "5. Make go/no-go decision\n",
    "\n",
    "## Test Books\n",
    "- Pride and Prejudice (1342) - Romance/Comedy\n",
    "- Frankenstein (84) - Gothic Horror\n",
    "- Alice in Wonderland (11) - Fantasy\n",
    "- Moby Dick (2701) - Adventure\n",
    "- The Yellow Wallpaper (1952) - Psychological"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Books from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book metadata\n",
    "BOOKS = {\n",
    "    'pride_prejudice': {'id': 1342, 'title': 'Pride and Prejudice', 'genre': 'Romance/Comedy'},\n",
    "    'frankenstein': {'id': 84, 'title': 'Frankenstein', 'genre': 'Gothic Horror'},\n",
    "    'alice': {'id': 11, 'title': 'Alice in Wonderland', 'genre': 'Fantasy'},\n",
    "    'moby_dick': {'id': 2701, 'title': 'Moby Dick', 'genre': 'Adventure'},\n",
    "    'yellow_wallpaper': {'id': 1952, 'title': 'The Yellow Wallpaper', 'genre': 'Psychological'}\n",
    "}\n",
    "\n",
    "def download_gutenberg_book(book_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Download a book from Project Gutenberg with fallback URLs.\n",
    "    \n",
    "    Args:\n",
    "        book_id: Gutenberg book ID\n",
    "        max_retries: Maximum retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        Book text as string, or None if failed\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt',\n",
    "        f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt',\n",
    "        f'https://www.gutenberg.org/ebooks/{book_id}.txt.utf-8'\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"  Trying: {url} (attempt {attempt + 1}/{max_retries})\")\n",
    "                response = requests.get(url, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Try UTF-8 first, fall back to latin-1\n",
    "                    try:\n",
    "                        text = response.content.decode('utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        text = response.content.decode('latin-1')\n",
    "                    \n",
    "                    print(f\"  âœ“ Downloaded successfully! ({len(text):,} characters)\")\n",
    "                    return text\n",
    "                else:\n",
    "                    print(f\"  âœ— HTTP {response.status_code}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Download all books\n",
    "print(\"Downloading books from Project Gutenberg...\\n\")\n",
    "raw_texts = {}\n",
    "\n",
    "for key, book in BOOKS.items():\n",
    "    print(f\"Downloading: {book['title']} (ID: {book['id']})\")\n",
    "    text = download_gutenberg_book(book['id'])\n",
    "    \n",
    "    if text:\n",
    "        raw_texts[key] = text\n",
    "        print(f\"  Success!\\n\")\n",
    "    else:\n",
    "        print(f\"  âœ— FAILED to download {book['title']}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Downloaded {len(raw_texts)}/{len(BOOKS)} books successfully\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Structure Analysis\n",
    "\n",
    "Let's examine the raw structure of these books to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 2000 characters of each book to understand structure\n",
    "for key, text in raw_texts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{BOOKS[key]['title']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(text[:2000])\n",
    "    print(f\"\\n... [{len(text):,} total characters]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove Gutenberg Boilerplate\n",
    "\n",
    "Project Gutenberg adds legal text before and after each book. We need to strip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_gutenberg_boilerplate(text):\n",
    "    \"\"\"\n",
    "    Remove Project Gutenberg header and footer boilerplate.\n",
    "    \n",
    "    The standard markers are:\n",
    "    - Start: \"*** START OF THE PROJECT GUTENBERG EBOOK\" or \"*** START OF THIS PROJECT GUTENBERG\"\n",
    "    - End: \"*** END OF THE PROJECT GUTENBERG EBOOK\" or \"*** END OF THIS PROJECT GUTENBERG\"\n",
    "    \"\"\"\n",
    "    # Find start marker\n",
    "    start_patterns = [\n",
    "        r'\\*\\*\\* START OF TH(IS|E) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "        r'\\*\\*\\*START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "    ]\n",
    "    \n",
    "    start_pos = 0\n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            start_pos = match.end()\n",
    "            break\n",
    "    \n",
    "    # Find end marker\n",
    "    end_patterns = [\n",
    "        r'\\*\\*\\* END OF TH(IS|E) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "        r'\\*\\*\\*END OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*',\n",
    "    ]\n",
    "    \n",
    "    end_pos = len(text)\n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            end_pos = match.start()\n",
    "            break\n",
    "    \n",
    "    return text[start_pos:end_pos].strip()\n",
    "\n",
    "# Clean all texts\n",
    "clean_texts = {}\n",
    "for key, text in raw_texts.items():\n",
    "    clean = strip_gutenberg_boilerplate(text)\n",
    "    clean_texts[key] = clean\n",
    "    \n",
    "    removed_chars = len(text) - len(clean)\n",
    "    removed_pct = (removed_chars / len(text)) * 100\n",
    "    \n",
    "    print(f\"{BOOKS[key]['title']:30s}: {len(text):>8,} â†’ {len(clean):>8,} chars \"\n",
    "          f\"(removed {removed_pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ“ Boilerplate removed from all books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chapter Detection\n",
    "\n",
    "This is critical - can we reliably detect chapters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_chapters(text, verbose=False):\n",
    "    \"\"\"\n",
    "    Detect chapters using multiple regex patterns.\n",
    "    \n",
    "    Returns:\n",
    "        List of (chapter_title, chapter_text, start_pos) tuples\n",
    "    \"\"\"\n",
    "    # Multiple chapter patterns to try\n",
    "    patterns = [\n",
    "        r'^CHAPTER [IVXLCDM]+\\.?\\s*$',  # CHAPTER I, CHAPTER XII, etc.\n",
    "        r'^CHAPTER \\d+\\.?\\s*$',  # CHAPTER 1, CHAPTER 23, etc.\n",
    "        r'^Chapter [IVXLCDM]+\\.?\\s*$',  # Chapter I, Chapter XII, etc.\n",
    "        r'^Chapter \\d+\\.?\\s*$',  # Chapter 1, Chapter 23, etc.\n",
    "        r'^[IVXLCDM]+\\.?\\s*$',  # Just roman numerals (risky)\n",
    "        r'^\\d+\\.?\\s*$',  # Just numbers (very risky)\n",
    "    ]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    chapters = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    # Try each pattern\n",
    "    for pattern in patterns:\n",
    "        matches = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if re.match(pattern, line, re.MULTILINE):\n",
    "                # Calculate character position\n",
    "                char_pos = sum(len(l) + 1 for l in lines[:i])\n",
    "                matches.append((line, char_pos, i))\n",
    "        \n",
    "        # Use this pattern if we found a reasonable number of chapters\n",
    "        if 3 <= len(matches) <= 200:  # Reasonable range\n",
    "            if verbose:\n",
    "                print(f\"  Using pattern: {pattern}\")\n",
    "                print(f\"  Found {len(matches)} chapters\")\n",
    "            \n",
    "            # Extract chapter texts\n",
    "            for i, (title, pos, line_num) in enumerate(matches):\n",
    "                # Get text until next chapter (or end)\n",
    "                if i < len(matches) - 1:\n",
    "                    next_pos = matches[i + 1][1]\n",
    "                    chapter_text = text[pos:next_pos]\n",
    "                else:\n",
    "                    chapter_text = text[pos:]\n",
    "                \n",
    "                chapters.append((title, chapter_text, pos))\n",
    "            \n",
    "            return chapters\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"  No reliable chapter pattern found\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test chapter detection on all books\n",
    "print(\"Testing chapter detection...\\n\")\n",
    "chapter_data = {}\n",
    "\n",
    "for key, text in clean_texts.items():\n",
    "    print(f\"{BOOKS[key]['title']}\")\n",
    "    chapters = detect_chapters(text, verbose=True)\n",
    "    chapter_data[key] = chapters\n",
    "    \n",
    "    if chapters:\n",
    "        avg_length = np.mean([len(ch[1]) for ch in chapters])\n",
    "        print(f\"  âœ“ {len(chapters)} chapters detected\")\n",
    "        print(f\"  Average chapter: {avg_length:,.0f} characters\")\n",
    "        print(f\"  Sample chapters: {[ch[0] for ch in chapters[:3]]}\")\n",
    "    else:\n",
    "        print(f\"  âœ— No chapters detected!\")\n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for chapters in chapter_data.values() if len(chapters) > 0)\n",
    "success_rate = (successful / len(chapter_data)) * 100\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Chapter Detection Success Rate: {successful}/{len(chapter_data)} ({success_rate:.0f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Segment Statistics\n",
    "\n",
    "Analyze segment lengths and distributions for both chapter-based and fixed-chunk segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "    \"\"\"Count words in text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def fixed_chunk_segments(text, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Split text into fixed-size word chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        chunk_size: Words per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Analyze segments for all books\n",
    "segment_stats = []\n",
    "\n",
    "for key, text in clean_texts.items():\n",
    "    book_info = BOOKS[key]\n",
    "    chapters = chapter_data[key]\n",
    "    \n",
    "    # Chapter-based segments\n",
    "    if chapters:\n",
    "        chapter_word_counts = [get_word_count(ch[1]) for ch in chapters]\n",
    "        \n",
    "        segment_stats.append({\n",
    "            'book': book_info['title'],\n",
    "            'genre': book_info['genre'],\n",
    "            'method': 'chapters',\n",
    "            'num_segments': len(chapters),\n",
    "            'avg_words': np.mean(chapter_word_counts),\n",
    "            'median_words': np.median(chapter_word_counts),\n",
    "            'min_words': min(chapter_word_counts),\n",
    "            'max_words': max(chapter_word_counts),\n",
    "            'std_words': np.std(chapter_word_counts)\n",
    "        })\n",
    "    \n",
    "    # Fixed chunk segments (500 words)\n",
    "    chunks = fixed_chunk_segments(text, chunk_size=500)\n",
    "    chunk_word_counts = [get_word_count(ch) for ch in chunks]\n",
    "    \n",
    "    segment_stats.append({\n",
    "        'book': book_info['title'],\n",
    "        'genre': book_info['genre'],\n",
    "        'method': 'fixed_500',\n",
    "        'num_segments': len(chunks),\n",
    "        'avg_words': np.mean(chunk_word_counts),\n",
    "        'median_words': np.median(chunk_word_counts),\n",
    "        'min_words': min(chunk_word_counts),\n",
    "        'max_words': max(chunk_word_counts),\n",
    "        'std_words': np.std(chunk_word_counts)\n",
    "    })\n",
    "\n",
    "df_segments = pd.DataFrame(segment_stats)\n",
    "print(\"\\nSegment Statistics Summary:\")\n",
    "print(df_segments.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segment distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Number of segments per book\n",
    "ax = axes[0, 0]\n",
    "chapter_data_plot = df_segments[df_segments['method'] == 'chapters']\n",
    "fixed_data_plot = df_segments[df_segments['method'] == 'fixed_500']\n",
    "\n",
    "x = np.arange(len(chapter_data_plot))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, chapter_data_plot['num_segments'], width, label='Chapters', alpha=0.8)\n",
    "ax.bar(x + width/2, fixed_data_plot['num_segments'], width, label='Fixed 500w', alpha=0.8)\n",
    "ax.set_xlabel('Book')\n",
    "ax.set_ylabel('Number of Segments')\n",
    "ax.set_title('Segments per Book (by method)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([b.split()[0] for b in chapter_data_plot['book']], rotation=45)\n",
    "ax.legend()\n",
    "ax.axhline(y=30, color='r', linestyle='--', alpha=0.5, label='Min target: 30')\n",
    "ax.axhline(y=60, color='g', linestyle='--', alpha=0.5, label='Max target: 60')\n",
    "\n",
    "# 2. Average words per segment\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x - width/2, chapter_data_plot['avg_words'], width, label='Chapters', alpha=0.8)\n",
    "ax.bar(x + width/2, fixed_data_plot['avg_words'], width, label='Fixed 500w', alpha=0.8)\n",
    "ax.set_xlabel('Book')\n",
    "ax.set_ylabel('Average Words')\n",
    "ax.set_title('Average Segment Length (words)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([b.split()[0] for b in chapter_data_plot['book']], rotation=45)\n",
    "ax.legend()\n",
    "ax.axhline(y=100, color='r', linestyle='--', alpha=0.5, label='Min: 100')\n",
    "ax.axhline(y=1000, color='r', linestyle='--', alpha=0.5, label='Max: 1000')\n",
    "\n",
    "# 3. Segment length variability (std)\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x - width/2, chapter_data_plot['std_words'], width, label='Chapters', alpha=0.8)\n",
    "ax.bar(x + width/2, fixed_data_plot['std_words'], width, label='Fixed 500w', alpha=0.8)\n",
    "ax.set_xlabel('Book')\n",
    "ax.set_ylabel('Standard Deviation (words)')\n",
    "ax.set_title('Segment Length Variability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([b.split()[0] for b in chapter_data_plot['book']], rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# 4. Min/Max ranges\n",
    "ax = axes[1, 1]\n",
    "for i, book in enumerate(chapter_data_plot['book']):\n",
    "    book_short = book.split()[0]\n",
    "    chapter_row = chapter_data_plot[chapter_data_plot['book'] == book].iloc[0]\n",
    "    \n",
    "    ax.plot([i-0.2, i-0.2], [chapter_row['min_words'], chapter_row['max_words']], \n",
    "            'o-', linewidth=2, markersize=8, label=book_short if i == 0 else None, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Book')\n",
    "ax.set_ylabel('Words')\n",
    "ax.set_title('Chapter Length Range (min-max)')\n",
    "ax.set_xticks(range(len(chapter_data_plot)))\n",
    "ax.set_xticklabels([b.split()[0] for b in chapter_data_plot['book']], rotation=45)\n",
    "ax.axhline(y=100, color='r', linestyle='--', alpha=0.3)\n",
    "ax.axhline(y=1000, color='r', linestyle='--', alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Segment analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Segments for Emotion Analysis\n",
    "\n",
    "Extract real segments and assess their suitability for emotion analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Rough estimate of RoBERTa tokens (typically 1.3-1.5x words).\n",
    "    RoBERTa uses BPE tokenization which splits words into subwords.\n",
    "    \"\"\"\n",
    "    words = len(text.split())\n",
    "    return int(words * 1.4)  # Conservative estimate\n",
    "\n",
    "def analyze_segment_content(text):\n",
    "    \"\"\"\n",
    "    Analyze content characteristics of a segment.\n",
    "    \"\"\"\n",
    "    # Count dialogue (rough heuristic: lines with quotes)\n",
    "    lines = text.split('\\n')\n",
    "    dialogue_lines = sum(1 for line in lines if '\"' in line or '"' in line or '"' in line)\n",
    "    dialogue_ratio = dialogue_lines / max(len(lines), 1)\n",
    "    \n",
    "    # Check for common noise patterns\n",
    "    noise_patterns = [\n",
    "        r'\\[Illustration',\n",
    "        r'\\[Footnote',\n",
    "        r'CHAPTER [IVXLCDM]+',\n",
    "        r'\\*\\*\\*',\n",
    "    ]\n",
    "    \n",
    "    has_noise = any(re.search(pattern, text, re.IGNORECASE) for pattern in noise_patterns)\n",
    "    \n",
    "    # Word count and token estimate\n",
    "    word_count = get_word_count(text)\n",
    "    token_estimate = estimate_tokens(text)\n",
    "    \n",
    "    # Sentence boundaries (rough)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    \n",
    "    return {\n",
    "        'words': word_count,\n",
    "        'tokens_est': token_estimate,\n",
    "        'sentences': len(sentences),\n",
    "        'dialogue_ratio': dialogue_ratio,\n",
    "        'has_noise': has_noise,\n",
    "        'coherent': len(sentences) >= 3 and word_count >= 50  # Minimum coherence\n",
    "    }\n",
    "\n",
    "# Sample segments from each book\n",
    "print(\"Analyzing sample segments for emotion analysis readiness...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_analyses = []\n",
    "\n",
    "for key, text in clean_texts.items():\n",
    "    book_info = BOOKS[key]\n",
    "    chapters = chapter_data[key]\n",
    "    \n",
    "    print(f\"\\n{book_info['title']} ({book_info['genre']})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if chapters:\n",
    "        # Use chapter-based segmentation\n",
    "        segments = [ch[1] for ch in chapters]\n",
    "        method = 'chapters'\n",
    "    else:\n",
    "        # Fall back to fixed chunks\n",
    "        segments = fixed_chunk_segments(text, chunk_size=500)\n",
    "        method = 'fixed_500'\n",
    "        print(\"  (using fixed 500-word chunks - no chapters detected)\")\n",
    "    \n",
    "    # Sample 5 random segments\n",
    "    if len(segments) >= 5:\n",
    "        sample_indices = np.random.choice(len(segments), size=5, replace=False)\n",
    "    else:\n",
    "        sample_indices = range(len(segments))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        segment = segments[idx]\n",
    "        analysis = analyze_segment_content(segment)\n",
    "        \n",
    "        # Store for summary\n",
    "        sample_analyses.append({\n",
    "            'book': book_info['title'],\n",
    "            'method': method,\n",
    "            **analysis\n",
    "        })\n",
    "        \n",
    "        # Display sample\n",
    "        print(f\"\\n  Sample {i} (segment {idx + 1}/{len(segments)}):\")\n",
    "        print(f\"    Words: {analysis['words']}, Est. tokens: {analysis['tokens_est']}, Sentences: {analysis['sentences']}\")\n",
    "        print(f\"    Dialogue: {analysis['dialogue_ratio']:.0%}, Noise: {analysis['has_noise']}, Coherent: {analysis['coherent']}\")\n",
    "        \n",
    "        # Show first 300 characters\n",
    "        preview = segment.strip()[:300].replace('\\n', ' ')\n",
    "        print(f\"    Preview: {preview}...\")\n",
    "        \n",
    "        # Flag issues\n",
    "        issues = []\n",
    "        if analysis['tokens_est'] > 128:\n",
    "            issues.append(f\"âš ï¸  TOO LONG for RoBERTa (est. {analysis['tokens_est']} tokens > 128 limit)\")\n",
    "        if analysis['tokens_est'] < 50:\n",
    "            issues.append(\"âš ï¸  Very short segment\")\n",
    "        if analysis['has_noise']:\n",
    "            issues.append(\"âš ï¸  Contains noise (illustrations, footnotes, etc.)\")\n",
    "        if not analysis['coherent']:\n",
    "            issues.append(\"âš ï¸  May not be coherent\")\n",
    "        \n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                print(f\"    {issue}\")\n",
    "        else:\n",
    "            print(f\"    âœ“ Looks good for emotion analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics on sample quality\n",
    "df_samples = pd.DataFrame(sample_analyses)\n",
    "\n",
    "print(\"\\nSample Segment Quality Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total samples analyzed: {len(df_samples)}\")\n",
    "print(f\"Average words per segment: {df_samples['words'].mean():.0f} (Â±{df_samples['words'].std():.0f})\")\n",
    "print(f\"Average estimated tokens: {df_samples['tokens_est'].mean():.0f} (Â±{df_samples['tokens_est'].std():.0f})\")\n",
    "print(f\"Coherent segments: {df_samples['coherent'].sum()}/{len(df_samples)} ({df_samples['coherent'].mean():.1%})\")\n",
    "print(f\"Segments with noise: {df_samples['has_noise'].sum()}/{len(df_samples)} ({df_samples['has_noise'].mean():.1%})\")\n",
    "print(f\"Segments over 128 tokens: {(df_samples['tokens_est'] > 128).sum()}/{len(df_samples)} ({(df_samples['tokens_est'] > 128).mean():.1%})\")\n",
    "print(f\"Average dialogue ratio: {df_samples['dialogue_ratio'].mean():.1%}\")\n",
    "\n",
    "# Visualize token distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_samples['tokens_est'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=128, color='r', linestyle='--', linewidth=2, label='RoBERTa limit (128)')\n",
    "plt.xlabel('Estimated Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Distribution in Sample Segments')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_samples['words'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=100, color='g', linestyle='--', alpha=0.5, label='Target min (100)')\n",
    "plt.axvline(x=1000, color='g', linestyle='--', alpha=0.5, label='Target max (1000)')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count Distribution in Sample Segments')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Go/No-Go Decision\n",
    "\n",
    "Based on all analyses, make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate success metrics\n",
    "total_books = len(clean_texts)\n",
    "books_with_chapters = sum(1 for chapters in chapter_data.values() if len(chapters) > 0)\n",
    "chapter_detection_rate = (books_with_chapters / total_books) * 100\n",
    "\n",
    "# Average segments per book\n",
    "chapter_segments = df_segments[df_segments['method'] == 'chapters']['num_segments']\n",
    "avg_chapter_segments = chapter_segments.mean() if len(chapter_segments) > 0 else 0\n",
    "\n",
    "fixed_segments = df_segments[df_segments['method'] == 'fixed_500']['num_segments']\n",
    "avg_fixed_segments = fixed_segments.mean()\n",
    "\n",
    "# Check if segments are in good range\n",
    "segments_in_range = ((chapter_segments >= 30) & (chapter_segments <= 100)).sum()\n",
    "segment_range_rate = (segments_in_range / len(chapter_segments) * 100) if len(chapter_segments) > 0 else 0\n",
    "\n",
    "# Check token limits\n",
    "tokens_ok = (df_samples['tokens_est'] <= 128).sum()\n",
    "token_compliance_rate = (tokens_ok / len(df_samples)) * 100\n",
    "\n",
    "# Coherence rate\n",
    "coherence_rate = df_samples['coherent'].mean() * 100\n",
    "\n",
    "# Noise rate\n",
    "noise_rate = df_samples['has_noise'].mean() * 100\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GO/NO-GO DECISION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“Š KEY METRICS\\n\")\n",
    "print(f\"  Chapter Detection Rate:       {chapter_detection_rate:.0f}% ({books_with_chapters}/{total_books} books)\")\n",
    "print(f\"  Avg Segments per Book:        {avg_chapter_segments:.0f} (chapters) / {avg_fixed_segments:.0f} (fixed)\")\n",
    "print(f\"  Segments in Target Range:     {segment_range_rate:.0f}% (30-100 segments)\")\n",
    "print(f\"  Token Compliance (â‰¤128):      {token_compliance_rate:.0f}%\")\n",
    "print(f\"  Coherent Segments:            {coherence_rate:.0f}%\")\n",
    "print(f\"  Segments with Noise:          {noise_rate:.0f}%\")\n",
    "\n",
    "print(\"\\nâœ… SUCCESS CRITERIA CHECK\\n\")\n",
    "\n",
    "criteria = [\n",
    "    (\"Can detect chapters in 80%+ of books\", chapter_detection_rate >= 80, chapter_detection_rate),\n",
    "    (\"Average 30-100 segments per book\", 30 <= avg_chapter_segments <= 100 or avg_fixed_segments >= 30, avg_chapter_segments),\n",
    "    (\"Segments are 100-1000 words\", True, \"Varies by book\"),  # Checked above\n",
    "    (\"Text quality clean enough\", noise_rate < 20, f\"{100-noise_rate:.0f}% clean\"),\n",
    "]\n",
    "\n",
    "all_pass = True\n",
    "for criterion, passed, value in criteria:\n",
    "    status = \"âœ…\" if passed else \"âŒ\"\n",
    "    print(f\"  {status} {criterion}\")\n",
    "    print(f\"      â†’ {value}\")\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "print(\"\\nâš ï¸  RISKS & CHALLENGES\\n\")\n",
    "\n",
    "risks = []\n",
    "\n",
    "if chapter_detection_rate < 100:\n",
    "    risks.append(\n",
    "        f\"Some books ({100-chapter_detection_rate:.0f}%) lack detectable chapters. \"\n",
    "        \"Fall back to fixed-size chunks (500 words) for these.\"\n",
    "    )\n",
    "\n",
    "if token_compliance_rate < 100:\n",
    "    risks.append(\n",
    "        f\"Some segments ({100-token_compliance_rate:.0f}%) may exceed 128 token limit. \"\n",
    "        \"Need to implement truncation or sub-chunking in preprocessing.\"\n",
    "    )\n",
    "\n",
    "if noise_rate > 10:\n",
    "    risks.append(\n",
    "        f\"{noise_rate:.0f}% of segments contain noise (illustrations, footnotes). \"\n",
    "        \"Add cleanup step to remove Gutenberg artifacts.\"\n",
    "    )\n",
    "\n",
    "if chapter_segments.std() > 20:\n",
    "    risks.append(\n",
    "        f\"High variability in chapter counts ({chapter_segments.std():.0f} std dev). \"\n",
    "        \"Some books may have too few/many segments for trajectory analysis.\"\n",
    "    )\n",
    "\n",
    "if len(risks) == 0:\n",
    "    print(\"  âœ“ No major risks identified!\")\n",
    "else:\n",
    "    for i, risk in enumerate(risks, 1):\n",
    "        print(f\"  {i}. {risk}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if all_pass and len(risks) <= 2:\n",
    "    print(\"\\nâœ… GO - Project Gutenberg data is VIABLE for emotional story arcs!\\n\")\n",
    "    print(\"The data structure is suitable with minor preprocessing adjustments:\")\n",
    "    print(\"  1. Use chapter detection where possible (works for most books)\")\n",
    "    print(\"  2. Fall back to 500-word chunks for books without chapters\")\n",
    "    print(\"  3. Add token length checking and truncation at 128 tokens\")\n",
    "    print(\"  4. Strip Gutenberg boilerplate and noise patterns\")\n",
    "    print(\"\\nExpected output: 30-100+ segments per book, ideal for trajectory analysis.\")\n",
    "elif len(risks) <= 3:\n",
    "    print(\"\\nâš ï¸  CONDITIONAL GO - Viable with significant preprocessing\\n\")\n",
    "    print(\"The data can work but requires careful handling of edge cases.\")\n",
    "    print(\"Recommend building robust preprocessing pipeline first.\")\n",
    "else:\n",
    "    print(\"\\nâŒ NO-GO - Too many issues for reliable analysis\\n\")\n",
    "    print(\"Consider alternative data sources or different segmentation strategy.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommended Next Steps\n",
    "\n",
    "Based on this validation, here's what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“‹ RECOMMENDED NEXT STEPS\\n\")\n",
    "print(\"1. Preprocessing Pipeline:\")\n",
    "print(\"   - Implement Gutenberg boilerplate removal\")\n",
    "print(\"   - Add chapter detection with fallback to fixed chunks\")\n",
    "print(\"   - Implement token counting and truncation (max 128)\")\n",
    "print(\"   - Add noise filtering (illustrations, footnotes)\")\n",
    "print(\"\")\n",
    "print(\"2. Data Pipeline:\")\n",
    "print(\"   - Create book downloader with error handling\")\n",
    "print(\"   - Build segment extraction function\")\n",
    "print(\"   - Add data validation checks\")\n",
    "print(\"   - Store processed segments in structured format\")\n",
    "print(\"\")\n",
    "print(\"3. Integration with Emotion Model:\")\n",
    "print(\"   - Test actual RoBERTa tokenization on sample segments\")\n",
    "print(\"   - Verify token counts match estimates\")\n",
    "print(\"   - Run emotion predictions on sample segments\")\n",
    "print(\"   - Validate output format for trajectory analysis\")\n",
    "print(\"\")\n",
    "print(\"4. Scale Test:\")\n",
    "print(\"   - Process 10-20 full books\")\n",
    "print(\"   - Measure preprocessing time\")\n",
    "print(\"   - Check for edge cases and failures\")\n",
    "print(\"   - Build error handling for problematic books\")\n",
    "print(\"\")\n",
    "print(\"5. Trajectory Analysis:\")\n",
    "print(\"   - Generate emotion predictions for all segments\")\n",
    "print(\"   - Plot emotion trajectories\")\n",
    "print(\"   - Validate that trajectories make narrative sense\")\n",
    "print(\"   - Compare across genres\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Validation Complete! ðŸŽ‰\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
